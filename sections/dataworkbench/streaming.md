---
author: Veracity
description: This page contains an overview of data streaming in Data Workbench.
---
# Data streaming
The streaming feature expands upon the existing Data Workbench's BYOD (Bring Your Own Data) feature and succeeds Veracity IoT Platform, which became a legacy software.


Streaming gives you a managed solution that supports the following needs:

* Continuous ingest of standardized data.
* Long-term storage of large and growing data sets (historian).
* Basic Query.
* Data Sharing.
* Connectivity with external analytics tools (for example, Spark).

Note that:
* Streaming is currently in the MVP (Minimum Viable Product) state.
* Streaming supports migrating customer data from the legacy Veracity IoT platform (Time Series) subject to standard hourly rates.
* Streaming is available on request. If you want to use it, go through the setup process described below.

## Use case scenarios for data streaming

### When to stream data
Below are use case scenarios when streaming data is recommended.

**Preventive Maintenance of Machines and Devices:**
* Streaming real-time data from sensors and devices allows organizations to monitor equipment health continuously.
* By analyzing streaming data, anomalies can be detected early, enabling timely maintenance or repairs.
* For instance, in an oil exploration and drilling company, streaming data from drilling equipment can help predict failures and prevent costly downtime.

**Real-Time Visibility and Control of Transportation, Logistics, and Supply Chains:**
* Streaming data provides real-time insights into the movement of goods, vehicles, and shipments.
* Logistics companies can optimize routes, track shipments, and respond promptly to delays or disruptions.
* For example, a smart logistics system can adjust delivery routes based on real-time traffic conditions or weather forecasts.

### When not to stream data
Below are scenarios when streaming data might not be the best approach.

**Low-Frequency Data:**
* If the data generated by devices or sensors is infrequent (for example., once a day), streaming may not be necessary.
* In such cases, batch processing or periodic uploads suffice.

**High Bandwidth Costs:**
* Streaming data continuously can incur significant bandwidth costs, especially for large-scale deployments.
* Consider the trade-off between real-time insights and operational expenses.

**Data Privacy and Security Concerns:**
* Streaming sensitive data (for example, personal information) requires robust security measures.
* If security risks outweigh the benefits of real-time streaming, consider alternative approaches.

## How to start using data streaming
Data streaming is a feature of Data Workbench, so if you are not a customer yet, contact the [onboarding team](mailto:onboarding@veracity.com) to get your Data Workbench workspace and tenant.

1. Use a [standardized schema we provide](#schema) (recommended) for ingesting and streaming your data. Alternatively, ask the [onboarding team](mailto:onboarding@veracity.com) to help you define your own schema. We support SparkplugB and Crimson.
2. Contact the [onboarding team](mailto:onboarding@veracity.com) to install your schema (if it is a custom one) and to create dataset(s) for you.
3. Send data to your dataset(s) configured for streaming. See [code snippets here]() in C#, NodeJS, Python. (**We will add code snippets**)
4. [Query data](tutorial/tutorialq.md).

You can stream data using Azure IoT Hub (MQTT, AQMP, HTTPS) or Azure Event Hub (AQMP, HTTPS).

You might want to see:
* [Azure Event Hub libraries for Pyhton](https://learn.microsoft.com/en-us/python/api/overview/azure/event-hubs?view=azure-python)
* [Send events to and receive events from Azure Event Hubs using .NET](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-dotnet-standard-getstarted-send?tabs=passwordless%2Croles-azure-portal)
* [Use Java to send events to or receive events from Azure Event Hubs](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-java-get-started-send?tabs=passwordless%2Croles-azure-portal).

## Best practices for sending data
This section covers best practices for sending data.

(**We will add best practices for sending data**)

### Standardized schema (time series)
<a name="schema"></a>This is an example of a standardize schema we recommend for streaming your data.

   | Attribute | Data Type | Description | Ingest Considerations |
   |--|--|--|--|
   | TimeStamp | datetime | Original timestamp of the datapoint. You decide how to handle time zones and ensure data consistency over time. We recommend sending it in UTC. | Mandatory. If missing, we drop the datapoint. You must send the data in ISO 8601 format; for example, `yyyy-MM-ddTHH:mm:ss.fffffffzzz`. Milliseconds are optional.
   | CorrelationId |string| An Id which is used to group datapoints to indicate they are related. You decide on ID scheme and relationship. | Optional |
   | AssetId |string| The identifier of the Asset the datapoint is for; important when data for multiple assets is being ingested to the same dataset. Can be any ID, IMO number, and so on.  | Optional |
   | DataChannelId |string| The identifier for the data channel, for example, "Speed", or an ID from a naming standard.| Optional |
   | DeviceId |string| Used to give context to the datapoint, for example, "Engine1", "Engine2". | Optional|
   | Value | string | The raw value. |Can be anything, numeric, non-numeric, but payload length is limited. Stored unmodified as a string. | Optional|
   | Quality | string| Metadata of the data point. Quality indication from the control system.| Optional|
   | Unit | string| Metadata of the data point. Unit (for example, degrees, rpm.)| Optional|
   | Type | string | Metadata of the data point. Depending on your use case, it can be a data type (for example, int, bool, string) or aggregation type (min, max, avg, hourly). | Optional. No processing logic is performed based on this value.|
   | _Received | datetime | Added by ingest job, cannot be set by sender. The datetime in UTC when the data was processed for ingest. |Some lag can occur, and there may be clock drift visible related to the TimeStamp attribute value.
   | _ValueNumeric |float| Added by ingest job, cannot be set by sender. The numeric representation of the Value. |If the Value can parse to numerical value, will be converted and stored as a floating point number. Some precision loss can occur. It cannot be parsed when attribute will be null.|

   ## Frequently asked questions

   ### How IoT Hub and Event Hub are different and which one to choose
  **Answer**: IoT Hub implements a daily message quota, whereas Event Hub implements bandwidth throttling. Event Hub supports HTTPS, AQMP and Kafka. IoT Hub supports HTTPS, AQMP and MQTT. Both are widely supported in SDKs and third party libraries and products.

  Both are highly scalable and the answer to the question which one to choose depends on your needs and unique situation.

  ### How to clean up data
  **Question**:  I have ingested some poor quality data, or duplicate data that I want to clean up. How do I do this?

  **Answer**:    You can't. We capture and store what is sent, as RAW data. The time-series data storage is append only and cannot be modified. The preparation step for your analytics should handle and filter for these scenarios. Alternatively you can talk to us about implementing a customized medallion architecture for your data lake. Also, please do not send test data to your production environment. Please contact us if you need additional datasets for testing purposes.

  ### How to generate a replacement connection string
  **Question**: I have comprised my connection string / key. How can I generate a replacement?

  **Answer**: Contact support. They can revoke the key and provide you with a replacement.

  ### Throttling exception when sending data to EventHub
  **Question**: I am getting a Throttling Exception when sending data to my Event Hub. What does this mean?

  **Answer**: You are most likely experiencing a temporary spike in traffic which exceeds your throughput limitation (measured in messages/second or MB/second). Either try again later or contact us to upgrade your subscription. It is customer responsibility to ensure the sending solution is robust enough to handle this scenario and buffer/retry the data until the data is successfully sent.

  ### How many streaming data sets do I need
  **Answer**: It depends. We recommend 1 per schema, per workspace, per region. It is possible to ingest from multiple producers in to a single dataset using IoT Hub as long as they all send using the same schema. For example, if you have a fleet of 10 vessels, all 10 could stream to the same dataset using their own connection string. The dataset can then be filtered/shared as required. The sender should provide AssetId or other data to be able to distingush between the different sources of data.

  ### Can I use my custom ingest schema
  **Answer**: We highly recommend using our standard schemas for maximum compatibility with our services. However, yes, it is possible to use a customized schema. There are some limitations, for example the schema must have a Timestamp column (called "Timestamp", of type DateTime). Please contact us to discuss your specific requirements.

  ### Query performance
  **Question**: What type of query performance can I expect?

   **Answer**: The performance profile of data lake is different from what can be expected from e.g. SQL database. You should expect queries to take between 5 and 30 seconds depending on query complexity and volume of data being scanned. You should always filter on timestamp. If the queries become slower as the volume of data is growing, it can be that some housekeeping is needed in your datalake to ensure good performance. Please contact support for assistance.

   ### Supported timestamp precision
   **Question**: What is the precision of timestamp supported?

   **Answer**: We can ingest and store nano-second precision (e.g. 2024-06-10T09:10:34.473977Z). Note that BYOD file upload only supports milli-second precision, streaming supports nano-second precision.

